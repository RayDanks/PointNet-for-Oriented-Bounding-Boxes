{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNet_Implementation_and_Innovation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPqZGYvEL6HA4Bwar/qX0Q3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNMSzOL9GNuo",
        "colab_type": "text"
      },
      "source": [
        "# Raymond"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G41hg3wVsgnk",
        "colab_type": "text"
      },
      "source": [
        "Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3cD111MsjGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyrender\n",
        "!sudo apt update\n",
        "!sudo wget https://github.com/mmatl/travis_debs/raw/master/xenial/mesa_18.3.3-0.deb\n",
        "!sudo dpkg -i ./mesa_18.3.3-0.deb || true\n",
        "!sudo apt install -f\n",
        "!git clone https://github.com/mmatl/pyopengl.git\n",
        "!pip install ./pyopengl\n",
        "!git clone https://github.com/mmatl/pyrender.git\n",
        "!git clone https://github.com/RayDanks/Data_Download_PointNet.git #downloading my data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixJT-VlMpWc8",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uhjWjtIGQEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
        "import numpy as np\n",
        "import trimesh\n",
        "import pyrender\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten, BatchNormalization, Lambda, Input\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import Adam\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import Model\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "import random\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pyrender\n",
        "import tensorflow as tf\n",
        "import pandas\n",
        "import trimesh\n",
        "import glob\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMZkhuXHdE6G",
        "colab_type": "text"
      },
      "source": [
        "# Creating my own dataset for 3D Bounding boxes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYHShs_Mk1Md",
        "colab_type": "text"
      },
      "source": [
        "Loading mesh point clouds\n",
        "\n",
        "Firstly we need to get the pointclouds of the meshes we are using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDrrhDi3dKy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "desired_point_number = 5000\n",
        "\n",
        "list_of_clouds = glob.glob('Data_Download_PointNet/Custom_Dataset/*.obj')\n",
        "print(list_of_clouds)\n",
        "\n",
        "unique_clouds = {}\n",
        "unique_meshes = {}\n",
        "for cloud_name in list_of_clouds:\n",
        "  point_cloud_mesh = trimesh.load(cloud_name)\n",
        "  cloud, face_index = point_cloud_mesh.sample(desired_point_number, True)\n",
        "\n",
        "  normals = point_cloud_mesh.face_normals[face_index]\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  The above sample allows for all of the clouds to be the same size\n",
        "  Hopefully in the future, we could use less dense clouds\n",
        "  etc too and just reach the same number of points by replicating points.\n",
        "  \"\"\"\n",
        "\n",
        "  new = cloud_name.split(\"Custom_Dataset/\") \n",
        "  identifier_with_obj = new[-1]\n",
        "  identifier = identifier_with_obj.split(\".\")\n",
        "  identifier = identifier[0]\n",
        "\n",
        "  \"\"\"\n",
        "  It is pertinent to move the centroid of the clouds to the\n",
        "  origin, so that they can be adequately rotated\n",
        "  \"\"\"\n",
        "  cloud_original_centroid = np.c_[[np.mean(cloud[:,0]),np.mean(cloud[:,1]),np.mean(cloud[:,2])]]\n",
        "\n",
        "  Origin = [0,0,0]\n",
        "  Movement_Vector = np.c_[Origin[0] - cloud_original_centroid[0],\n",
        "                          Origin[1] - cloud_original_centroid[1],\n",
        "                          Origin[2] - cloud_original_centroid[2]]\n",
        "\n",
        "\n",
        "  mesh_at_origin = cloud + Movement_Vector\n",
        "\n",
        "\n",
        "  #normals don't need to be translated as they're unit direction vectors\n",
        "  mesh_and_normals = np.hstack((mesh_at_origin,normals))\n",
        "\n",
        "  unique_clouds[identifier] = mesh_and_normals\n",
        "  unique_meshes[identifier] = point_cloud_mesh\n",
        "print(unique_clouds['cube'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o0JQ8h1pYqb",
        "colab_type": "text"
      },
      "source": [
        "All of these meshes originally have their axes algined to the camera\n",
        "we should make a function to determine the bounding box (corners)\n",
        "of these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFtGBMrIpohS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Find_3D_Bounding_Boxes(cloud):\n",
        "  x = cloud[:,0]\n",
        "  y = cloud[:,1]\n",
        "  z = cloud[:,2]\n",
        "\n",
        "  max_x = np.amax(x)\n",
        "  max_y = np.amax(y)\n",
        "  max_z = np.amax(z)\n",
        "\n",
        "  min_x = np.amin(x)\n",
        "  min_y = np.amin(y)\n",
        "  min_z = np.amin(z)\n",
        "\n",
        "  \"\"\"\n",
        "  First 4 points of bounding box are bottom 4 corners, from\n",
        "  bottom left going clockwise and then the next 4\n",
        "  points are from the top 4 corners, starting from top\n",
        "  left going clockwise\n",
        "  \"\"\"\n",
        "\n",
        "  bbox = np.zeros((8,3))\n",
        "\n",
        "  bbox[0,:] = [min_x,min_y,min_z]\n",
        "  bbox[1,:] = [min_x,max_y,min_z]\n",
        "  bbox[2,:] = [max_x,max_y,min_z]\n",
        "  bbox[3,:] = [max_x,min_y,min_z]\n",
        "\n",
        "  bbox[4,:] = [min_x,min_y,max_z]\n",
        "  bbox[5,:] = [min_x,max_y,max_z]\n",
        "  bbox[6,:] = [max_x,max_y,max_z]\n",
        "  bbox[7,:] = [max_x,min_y,max_z]\n",
        "\n",
        "  return bbox\n",
        "\n",
        "print(Find_3D_Bounding_Boxes(unique_clouds['cube']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqHMoWqosRoq",
        "colab_type": "text"
      },
      "source": [
        "We then use this to calculate the original bounding boxes of the clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHZj4ub_sfd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_bboxes = {}\n",
        "\n",
        "for ID, cloud_and_normals in unique_clouds.items():\n",
        "  cloud = cloud_and_normals[:,:3]\n",
        "  cloud_bbox = Find_3D_Bounding_Boxes(cloud)\n",
        "  unique_bboxes[ID] = cloud_bbox\n",
        "\n",
        "print(unique_bboxes['cube'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSlhGQVqtZDR",
        "colab_type": "text"
      },
      "source": [
        "Now we expand our dataset and make our system more resilient\n",
        "by rotating the original meshes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv2uUApMtgLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "amount_of_angles = 50 #each mesh has this number * 3 in the dataset\n",
        "angles = np.linspace(0,360,amount_of_angles) # in degrees - more intuitive\n",
        "amount_of_unique_clouds = len(unique_clouds)\n",
        "amount_of_overall_clouds = amount_of_angles*3*(amount_of_unique_clouds-1) #-1 for validation mesh\n",
        "cloud_dataset = np.zeros((amount_of_overall_clouds,desired_point_number,6))\n",
        "bbox_dataset = np.zeros((amount_of_overall_clouds,24)) #24; 8 points w/ XYZ \n",
        "#the boxes will be flattened so that they can be found regressively\n",
        "\n",
        "\n",
        "print(cloud_dataset.shape)\n",
        "print(bbox_dataset.shape)\n",
        "\n",
        "validation_mesh_ID = 'camel'\n",
        "validation_dataset = np.zeros((amount_of_angles*3,desired_point_number,6))\n",
        "validation_bbox_labels = np.zeros((amount_of_angles*3,24))\n",
        "\n",
        "print(validation_dataset.shape)\n",
        "print(validation_bbox_labels)\n",
        "\n",
        "tracker = 0 #tracks how many clouds\n",
        "val_tracker = 0 #validation tracker\n",
        "\n",
        "#x rotations\n",
        "\n",
        "for ID, cloud_and_normals in unique_clouds.items():\n",
        "  cloud = cloud_and_normals[:,:3]\n",
        "  normals = cloud_and_normals[:,3:] #we must rotate the normals too...\n",
        "  original_bbox = unique_bboxes[ID]\n",
        "  for a in angles:\n",
        "    a = np.deg2rad(a)\n",
        "    x_rot_mat = np.array(([1,0,0],[0,np.cos(a),-np.sin(a)],[0,np.sin(a),np.cos(a)]))\n",
        "    \n",
        "    rotated_cloud = np.dot(cloud,x_rot_mat.T)\n",
        "    rotated_normals = np.dot(normals,x_rot_mat.T)\n",
        "    rotated_bbox = np.dot(original_bbox, x_rot_mat.T)\n",
        "\n",
        "    rotated_cloud_and_normals = np.hstack((rotated_cloud,rotated_normals))\n",
        "    if ID == validation_mesh_ID:\n",
        "      val_tracker += 1\n",
        "      validation_dataset[val_tracker-1] = rotated_cloud_and_normals\n",
        "      validation_bbox_labels[val_tracker-1] = rotated_bbox.flatten()\n",
        "    else:\n",
        "      tracker += 1\n",
        "      cloud_dataset[tracker-1] = rotated_cloud_and_normals\n",
        "      bbox_dataset[tracker-1] = rotated_bbox.flatten()\n",
        "\n",
        "#y rotations\n",
        "for ID, cloud_and_normals in unique_clouds.items():\n",
        "  cloud = cloud_and_normals[:,:3]\n",
        "  normals = cloud_and_normals[:,3:]\n",
        "  original_bbox = unique_bboxes[ID]\n",
        "  for a in angles:\n",
        "    a = np.deg2rad(a)\n",
        "    y_rot_mat = np.array(([np.cos(a),0,np.sin(a)],[0,1,0],[-np.sin(a),0,np.cos(a)]))\n",
        "    \n",
        "    rotated_cloud = np.dot(cloud,y_rot_mat.T)\n",
        "    rotated_normals = np.dot(normals,y_rot_mat.T)\n",
        "    rotated_bbox = np.dot(original_bbox, y_rot_mat.T)\n",
        "\n",
        "    rotated_cloud_and_normals = np.hstack((rotated_cloud,rotated_normals))\n",
        "    if ID == validation_mesh_ID:\n",
        "      val_tracker += 1\n",
        "      validation_dataset[val_tracker-1] = rotated_cloud_and_normals\n",
        "      validation_bbox_labels[val_tracker-1] = rotated_bbox.flatten()\n",
        "    else:\n",
        "      tracker += 1\n",
        "      cloud_dataset[tracker-1] = rotated_cloud_and_normals\n",
        "      bbox_dataset[tracker-1] = rotated_bbox.flatten()\n",
        "\n",
        "\n",
        "#z rotations\n",
        "for ID, cloud_and_normals in unique_clouds.items():\n",
        "  cloud = cloud_and_normals[:,:3]\n",
        "  normals = cloud_and_normals[:,3:]\n",
        "  original_bbox = unique_bboxes[ID]\n",
        "  for a in angles:\n",
        "    a = np.deg2rad(a)\n",
        "    z_rot_mat = np.array(([np.cos(a),-np.sin(a),0],[np.sin(a),np.cos(a),0],[0,0,1]))\n",
        "    \n",
        "    rotated_cloud = np.dot(cloud,z_rot_mat.T)\n",
        "    rotated_normals = np.dot(normals,z_rot_mat.T)\n",
        "    rotated_bbox = np.dot(original_bbox, z_rot_mat.T)\n",
        "\n",
        "    rotated_cloud_and_normals = np.hstack((rotated_cloud,rotated_normals))\n",
        "    if ID == validation_mesh_ID:\n",
        "      val_tracker += 1\n",
        "      validation_dataset[val_tracker-1] = rotated_cloud_and_normals\n",
        "      validation_bbox_labels[val_tracker-1] = rotated_bbox.flatten()\n",
        "    else:\n",
        "      tracker += 1\n",
        "      cloud_dataset[tracker-1] = rotated_cloud_and_normals\n",
        "      bbox_dataset[tracker-1] = rotated_bbox.flatten()\n",
        "\n",
        "\n",
        "\n",
        "print(cloud_dataset)\n",
        "print(validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7TZi6a6yPzN",
        "colab_type": "text"
      },
      "source": [
        "Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlmvKNuYyRv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_mesh(mesh): #FROM PYRENDEER WEBSITE\n",
        "  mesh = pyrender.Mesh.from_trimesh(mesh)\n",
        "  scene = pyrender.Scene()\n",
        "  scene.add(mesh)\n",
        "\n",
        "  # Set up the camera -- z-axis away from the scene, x-axis right, y-axis up\n",
        "  camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0)\n",
        "  s = np.sqrt(2)/2\n",
        "  camera_pose = np.array([\n",
        "        [0.0, -s,   s,   0.3],\n",
        "        [1.0,  0.0, 0.0, 0.0],\n",
        "        [0.0,  s,   s,   0.35],\n",
        "        [0.0,  0.0, 0.0, 1.0],\n",
        "      ])\n",
        "  scene.add(camera, pose=camera_pose)\n",
        "\n",
        "  # Set up the light -- a single spot light in the same spot as the camera\n",
        "  light = pyrender.SpotLight(color=np.ones(3), intensity=3.0,\n",
        "                                innerConeAngle=np.pi/16.0)\n",
        "  scene.add(light, pose=camera_pose)\n",
        "\n",
        "  # Render the scene\n",
        "  r = pyrender.OffscreenRenderer(640, 480)\n",
        "  color, depth = r.render(scene)\n",
        "\n",
        "  # Show the images\n",
        "  plt.figure()\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(color)\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(depth, cmap=plt.cm.gray_r)\n",
        "  plt.show()\n",
        "\n",
        "show_mesh(unique_meshes['dragon'])\n",
        "\n",
        "\n",
        "def Plot_Mesh_Points_and_Bbox(mesh,bbox=None):\n",
        "  #bbox inputted via flattened corners\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "\n",
        "\n",
        "  ax1 = Axes3D(fig)\n",
        "  fig.set_figwidth(10)\n",
        "  ax1.scatter(mesh[:,0],mesh[:,1],mesh[:,2])\n",
        "  if bbox is not None:\n",
        "    bbox = bbox.reshape((8,3))\n",
        "    \n",
        "    #if we let matplotlib decide then the visual lines of the bbox will be wrong\n",
        "    c = 'red'\n",
        "    lw = 5\n",
        "\n",
        "    #plot the bottom four corners first:\n",
        "    ax1.plot(bbox[:4,0],bbox[:4,1],bbox[:4,2], c = c, linewidth=lw)\n",
        "\n",
        "    #straight line linking the final point to the first (bottom)\n",
        "    ax1.plot(bbox[(0,3),0],bbox[(0,3),1],bbox[(0,3),2], c = c, linewidth=lw)\n",
        "\n",
        "    #then plot the top 4 corners\n",
        "    ax1.plot(bbox[4:,0],bbox[4:,1],bbox[4:,2], c = c, linewidth=lw)\n",
        "\n",
        "    #straight line linking the final point to the first (top)\n",
        "    ax1.plot(bbox[(4,7),0],bbox[(4,7),1],bbox[(4,7),2], c = c, linewidth=lw)\n",
        "\n",
        "    #plot the 4 lines connecting the bottom and top\n",
        "    for i in range(4):\n",
        "      ax1.plot(bbox[(i,i+4),0],bbox[(i,i+4),1],bbox[(i,i+4),2], c = c, linewidth=lw)\n",
        "    \n",
        "\n",
        "\n",
        "  ax1.view_init(elev=25.)\n",
        "\n",
        "#Now we will plot some example clouds!\n",
        "\n",
        "example_1_cloud = unique_clouds['dragon']\n",
        "example_1_bbox = unique_bboxes['dragon']\n",
        "Plot_Mesh_Points_and_Bbox(example_1_cloud,example_1_bbox)\n",
        "\n",
        "\n",
        "example_2_cloud = cloud_dataset[amount_of_angles*3+amount_of_angles-1]\n",
        "example_2_bbox = bbox_dataset[amount_of_angles*3+amount_of_angles-1]\n",
        "Plot_Mesh_Points_and_Bbox(example_2_cloud,example_2_bbox)\n",
        "\n",
        "example_3_cloud = cloud_dataset[amount_of_angles*3+2]\n",
        "example_3_bbox = bbox_dataset[amount_of_angles*3+2]\n",
        "\n",
        "Plot_Mesh_Points_and_Bbox(example_3_cloud,example_3_bbox)\n",
        "\n",
        "\n",
        "example_4_cloud = validation_dataset[0]\n",
        "example_4_bbox = validation_bbox_labels[0]\n",
        "Plot_Mesh_Points_and_Bbox(example_4_cloud,example_4_bbox)\n",
        "\n",
        "example_5_cloud = validation_dataset[amount_of_angles]\n",
        "example_5_bbox = validation_bbox_labels[amount_of_angles]\n",
        "\n",
        "Plot_Mesh_Points_and_Bbox(example_5_cloud,example_5_bbox)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJOiPTJNg1YK",
        "colab_type": "text"
      },
      "source": [
        "Creating the PointNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xcaMK0mg3RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Final_Input_T_Layer(keras.layers.Layer):\n",
        "    def __init__(self, units=3*3, input_dim=256):\n",
        "        super(Final_Input_T_Layer, self).__init__()\n",
        "        w_init = tf.zeros_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"), #weights are 256x9 matrix\n",
        "            trainable=True,\n",
        "        )\n",
        "        b_init = tf.constant(np.eye(3,3).flatten(),dtype = \"float32\") #initial rotation is identity\n",
        "\n",
        "        self.b = tf.Variable(\n",
        "            initial_value=b_init, trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.reshape(tf.matmul(inputs, self.w) + self.b , (3,3))#return transformation matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PointNet_Model(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(PointNet_Model, self).__init__()\n",
        "    self.batch_size = 64 #from the paper\n",
        "    self.n = desired_point_number #derived from the data normalisation\n",
        "    self.number_of_segments = 5\n",
        "\n",
        "    self._Create_Point_Net_Layers()\n",
        "    # self._Create_Tranformation_Net_Layers()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def _Beginning_Segmentation_Layer_Func(self,x): \n",
        "    #this is a layer which manipluates the tensors such that we start the segmentation part of the model\n",
        "    \n",
        "    \n",
        "    #we can use tiling to maxe this tensor into a longer repeating matrix.\n",
        "\n",
        "    Max_Pool = x\n",
        "\n",
        "    self.filling_matrix = K.tile(Max_Pool,[1,self.n,1,1])\n",
        "\n",
        "    larger_matrix = K.concatenate((self.tensor_after_feature, self.filling_matrix), axis = 3) \n",
        "    #this is the matrix at the start of the segmentation section.\n",
        "\n",
        "    return larger_matrix\n",
        "\n",
        "  def _Input_Transformation_Function(self,x):\n",
        "    #this is the function for the layer which comes after all of the t net layers\n",
        "    #x = 256-long tensor\n",
        "\n",
        "\n",
        "    #firstly, multiply by the external weights.\n",
        "    weights_multiplied = K.dot(x,self.input_t_matrix_weights)\n",
        "\n",
        "    #weights multiplied = 9-long tensor\n",
        "\n",
        "    #Now add the external biases on.\n",
        "    biases_added = K.bias_add(weights_multiplied,self.input_t_matrix_biases)\n",
        "\n",
        "    #biases added = 9-long tensor\n",
        "    transformation_matrix = K.reshape(biases_added,shape = (3,3))\n",
        "\n",
        "    #this is now a 3x3 transformation matrix; transform the original input data\n",
        "    #during the forward pass\n",
        "\n",
        "    return transformation_matrix\n",
        "\n",
        "  def _Feature_Transformation_Function(self,x):\n",
        "    #x is 256-long tensor\n",
        "    \n",
        "    weights_multiplied = K.dot(x,self.feature_t_matrix_weights)\n",
        "\n",
        "    #weights multiplied = 64-long tensor\n",
        "\n",
        "    biases_added = K.bias_add(weights_multiplied,self.feature_t_matrix_biases)\n",
        "\n",
        "    #biases added = 9-long tensor\n",
        "    transformation_matrix = K.reshape(biases_added,shape = (64,64))\n",
        "\n",
        "    #this is now a 64x64 transformation matrix; transform the original input data\n",
        "    #during the forward pass\n",
        "\n",
        "    return transformation_matrix\n",
        "\n",
        "\n",
        "  def _Create_Tranformation_Net_Layers(self):\n",
        "    self.input_t_net_1 = Conv2D(64,kernel_size = (1,1), activation = 'relu', name = \"t_net_1\")\n",
        "    self.input_t_net_1_norm = BatchNormalization()\n",
        "\n",
        "    self.input_t_net_2 = Conv2D(128,kernel_size = (1,1), activation = 'relu', name = \"t_net_2\")\n",
        "    self.input_t_net_2_norm = BatchNormalization()\n",
        "\n",
        "    self.input_t_net_3 = Conv2D(1024,kernel_size = (1,1), activation = 'relu', name = \"t_net_3\")\n",
        "    self.input_t_net_3_norm = BatchNormalization()\n",
        "\n",
        "    self.input_t_net_pool = MaxPooling2D(pool_size = (self.n, 1), name = \"t_net_pool\")\n",
        "    self.input_t_net_pool_norm = BatchNormalization()\n",
        "\n",
        "    self.input_t_net_4 = Dense(512, activation = 'relu', name = \"t_net_4\")\n",
        "    self.input_t_net_4_norm = BatchNormalization()\n",
        "\n",
        "    self.input_t_net_5 = Dense(256, activation = 'relu', name = \"t_net_5\")\n",
        "    self.input_t_net_5_norm = BatchNormalization()\n",
        "\n",
        "    self.final_in_t_layer = Final_Input_T_Layer()\n",
        "\n",
        "  def _Create_Point_Net_Layers(self):\n",
        "\n",
        "    \"\"\" \n",
        "    This function is just creating each and every layer object,\n",
        "    these are individuals right now and become a graph in the \n",
        "    \"call\" function\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    #input dimensions is n x 3.\n",
        "    #after this, we do a T-Net transform\n",
        "    \n",
        "\n",
        "    #Going to assume ReLu activation and Batch Normalization is present:\n",
        "    \n",
        "\n",
        "    self.expand_dims = Lambda(lambda z: tf.expand_dims(z,-2), name = \"expand_dims_layer\")\n",
        "    # this allows for the x,y,z part of the points\n",
        "    #to just have dimension length 1.\n",
        "\n",
        "\n",
        "    self.first_output = Conv2D(64,kernel_size = (1,1), activation = 'relu', name = \"first_output\")\n",
        "    self.first_norm = BatchNormalization()\n",
        "\n",
        "    self.second_output = Conv2D(64,kernel_size = (1,1), activation = 'relu', name = \"second_output\")\n",
        "    self.second_norm = BatchNormalization()\n",
        "\n",
        "    #for feature transform\n",
        "    # self.tensor_after_feature = self.second_norm #for use in a lambda function\n",
        "\n",
        "\n",
        "\n",
        "    self.third_output = Conv2D(64,kernel_size = (1,1), activation = 'relu', name = \"third_output\")\n",
        "    self.third_norm = BatchNormalization()\n",
        "\n",
        "    self.fourth_output = Conv2D(128,kernel_size = (1,1), activation = 'relu', name = \"fourth_output\")\n",
        "    self.fourth_norm = BatchNormalization()\n",
        "\n",
        "    self.fifth_output = Conv2D(1024,kernel_size = (1,1), activation = 'relu', name = \"fifth_output\")\n",
        "    self.fifth_norm = BatchNormalization()\n",
        "\n",
        "    \"\"\"\n",
        "    now, similarly to the T-Net (since the T-Net is just a mini Point-Net)\n",
        "    we pool all of the features, in thr shape of the input points. \n",
        "    \"\"\"\n",
        "\n",
        "    self.Max_Pool_Output = MaxPooling2D(pool_size = (self.n, 1))\n",
        "    self.Max_Pool_Norm = BatchNormalization()\n",
        "\n",
        "    #Now the system should be 1024 long - we must concatenate this onto\n",
        "    #one of the previous layer's outputs, as shown in the diagram\n",
        "    #this layer becomes repeated rows (until the amount of rows = n).\n",
        "\n",
        "    self.start_of_segmentation = Lambda(self._Beginning_Segmentation_Layer_Func, name = \"Custom_Aggregation_Layer\")\n",
        "    self.start_of_segmentation.trainable = False\n",
        "\n",
        "    #Now we start the new sections of shared MLPs.\n",
        "\n",
        "    self.first_segment_output = Conv2D(512,kernel_size = (1,1), activation = 'relu', name = \"first_segment_output\")\n",
        "    self.first_segment_norm = BatchNormalization()\n",
        "\n",
        "    self.second_segment_output = Conv2D(256,kernel_size = (1,1), activation = 'relu', name = \"second_segment_output\")\n",
        "    self.second_segment_norm = BatchNormalization()\n",
        "\n",
        "    self.third_segment_output = Conv2D(128,kernel_size = (1,1), activation = 'relu', name = \"third_segment_output\")\n",
        "    self.third_segment_norm = BatchNormalization()\n",
        "    \n",
        "\n",
        "    #Now this is where the point features are (n x 128).\n",
        "    #need to pool to regress bounding box points\n",
        "\n",
        "    self.bbox_pool = MaxPooling2D(pool_size = (self.n, 1), name = 'Bounding_Box_Pool')\n",
        "    self.bbox_pool_norm = BatchNormalization()\n",
        "    #since the previous conv is 128, this is a 128 long tensor\n",
        "\n",
        "    self.bbox_regress_1 = Dense(64, activation = 'relu', name = 'First_Bounding_Box_Regression')\n",
        "    self.bbox_regress_1_norm = BatchNormalization()\n",
        "\n",
        "    self.bbox_regress_2 = Dense(32, activation = 'relu', name = 'Second_Bounding_Box_Regression')\n",
        "    self.bbox_regress_2_norm = BatchNormalization()\n",
        "\n",
        "    self.bbox_output = Dense(8*3, name = 'Bounding_Box_Output')\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,inputs):\n",
        "    \"\"\"\n",
        "    This function defines the model's/graph's forward pass.\n",
        "    All of the layer objects are already defined\n",
        "    and hence this layer is building them into a graph\n",
        "    \"\"\"\n",
        "\n",
        "    new_inputs = self.expand_dims(inputs)\n",
        "\n",
        "    #Start of input transformation net\n",
        "    # x = self.input_t_net_1(self.new_inputs)\n",
        "    # x = self.input_t_net_1_norm(x)\n",
        "    # x = self.input_t_net_2(x)\n",
        "    # x = self.input_t_net_2_norm(x)\n",
        "    # x = self.input_t_net_3(x)\n",
        "    # x = self.input_t_net_3_norm(x)\n",
        "    # x = self.input_t_net_pool(x)\n",
        "    # x = self.input_t_net_pool_norm(x)\n",
        "    # x = self.input_t_net_4(x)\n",
        "    # x = self.input_t_net_4_norm(x)\n",
        "    # x = self.input_t_net_5(x)\n",
        "    # x = self.input_t_net_5_norm(x)\n",
        "    # self.small_transform_matrix  = self.final_in_t_layer(x) #output is 3x3 matrix\n",
        "    # x = Lambda(lambda z: K.dot(self.new_inputs,z), \n",
        "    #            name = \"input_transform_multiplication\")(self.small_transform_matrix)\n",
        "    #End of input transformation net\n",
        "\n",
        "\n",
        "    x = self.first_output(new_inputs)\n",
        "    x = self.first_norm(x)\n",
        "    x = self.second_output(x)\n",
        "    x = self.second_norm(x)\n",
        "\n",
        "    #Start of feature transformation net\n",
        "    # x = self.feature_t_net_1(before_feature_transform)\n",
        "    # x = self.feature_t_net_1_norm(x)\n",
        "    # x = self.feature_t_net_2(x)\n",
        "    # x = self.feature_t_net_2_norm(x)\n",
        "    # x = self.feature_t_net_3(x)\n",
        "    # x = self.feature_t_net_3_norm(x)\n",
        "    # x = self.feature_t_net_pool(x)\n",
        "    # x = self.feature_t_net_pool_norm(x)\n",
        "    # x = self.feature_t_net_4(x)\n",
        "    # x = self.feature_t_net_4_norm(x)\n",
        "    # x = self.feature_t_net_5(x)\n",
        "    # x = self.feature_t_net_5_norm(x)\n",
        "    # large_transform_matrix = self.feature_transformation_matrix_layer(x) #output is 64x64 matrix\n",
        "    # x = Lambda(lambda z: K.dot(before_feature_transform,z),\n",
        "    #            name = \"feature_transform_multiplication\")(large_transform_matrix)\n",
        "    #End of feature transformation net\n",
        "\n",
        "    self.tensor_after_feature = x #necessary to use later during the custom layer\n",
        "    x = self.third_output(x)\n",
        "    x = self.third_norm(x)\n",
        "    x = self.fourth_output(x)\n",
        "    x = self.fourth_norm(x)\n",
        "    x = self.fifth_output(x)\n",
        "    x = self.fifth_norm(x)\n",
        "    x = self.Max_Pool_Output(x)\n",
        "    x = self.Max_Pool_Norm(x)\n",
        "    x = self.start_of_segmentation(x) #custom layer\n",
        "    x = self.first_segment_output(x)\n",
        "    x = self.first_segment_norm(x)\n",
        "    x = self.second_segment_output(x)\n",
        "    x = self.second_segment_norm(x)\n",
        "    x = self.third_segment_output(x)\n",
        "    x = self.third_segment_norm(x)\n",
        "\n",
        "    x = self.bbox_pool(x)\n",
        "    x = self.bbox_pool_norm(x)\n",
        "\n",
        "    x = self.bbox_regress_1(x)\n",
        "    x = self.bbox_regress_1_norm(x)\n",
        "\n",
        "    x = self.bbox_regress_2(x)\n",
        "    x = self.bbox_regress_2_norm(x)\n",
        "\n",
        "    x = self.bbox_output(x)\n",
        "\n",
        "    return x \n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giFSvyRr4fmE",
        "colab_type": "text"
      },
      "source": [
        "Compiling and Testing the Network with an Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhH7MfpRvj0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PointNet = PointNet_Model()\n",
        "input_layer = keras.layers.Input(shape = (PointNet.n,6))\n",
        "\n",
        "\n",
        "# PointNet.call(input_layer)\n",
        "\n",
        "Adam_Optimiser = keras.optimizers.Adam(lr = 0.001) #initial\n",
        "\n",
        "PointNet.compile(optimizer = Adam_Optimiser, loss = 'mse')\n",
        "\n",
        "\n",
        "print(PointNet.n)\n",
        "example_point = cloud_dataset[0]\n",
        "example_point = np.array(example_point).reshape((1,PointNet.n,6))\n",
        "\n",
        "\n",
        "print(\"example point\",example_point.shape)\n",
        "\n",
        "\n",
        "example_prediction = PointNet.predict(example_point)\n",
        "print(\"example prediction\",example_prediction)\n",
        "print(\"example prediction shape\",example_prediction.shape)\n",
        "PointNet.summary()\n",
        "print(example_prediction[0][0].shape)\n",
        "# print(PointNet.x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybEU3VsvXtcj",
        "colab_type": "text"
      },
      "source": [
        "# Training the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR1LjrhDy4uW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = PointNet.fit(cloud_dataset,\n",
        "                       bbox_dataset,\n",
        "                       batch_size = PointNet.batch_size,\n",
        "                       epochs = 200,\n",
        "                       verbose = 1,\n",
        "                       validation_data=(validation_dataset, validation_bbox_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAbv4DtYpQHX",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75l4ESKvo3tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saved results below are not the best that the system can do - it needs to train more\n",
        "\n",
        "example_1_cloud = cloud_dataset[amount_of_angles*3+amount_of_angles-1]\n",
        "example_2_cloud = cloud_dataset[amount_of_angles*3+2]\n",
        "\n",
        "\n",
        "example_1_cloud_reshape = np.array((cloud_dataset[amount_of_angles*3+amount_of_angles-1])).reshape((1,PointNet.n,6))\n",
        "example_2_cloud_reshape = np.array((cloud_dataset[amount_of_angles*3+2])).reshape((1,PointNet.n,6))\n",
        "\n",
        "print(example_1_cloud)\n",
        "print(example_1_cloud.shape)\n",
        "\n",
        "predicted_bbox_1 = PointNet.predict(example_1_cloud_reshape)\n",
        "predicted_bbox_2 = PointNet.predict(example_2_cloud_reshape)\n",
        "\n",
        "Plot_Mesh_Points_and_Bbox(example_1_cloud,predicted_bbox_1)\n",
        "Plot_Mesh_Points_and_Bbox(example_2_cloud,predicted_bbox_2)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}